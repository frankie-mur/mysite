[{"content":"Currently Reading\u0026hellip; Title Link Writing a Compiler in Go Link Acing the System Design Interview Link Have Read Title Thoughts Rating (out of 5) Link Writing An Intreptor In Go Really good book, and introduction to Go aswell as Programing languages, love how its written with a TDD approach 5/5 Link The Missing Read Me Awesome introduction to software engineering profession. Especialy enjoyed the chapers on SEMVER and comunicating with you manager 5/5 Link The Phoenix Project Classic. Good to take a break from dense techonology books 4/5 Link How Linux Works The introduction to Linux and Computer Systems 4/5 Link System Design Interview Vol.2 Read about halfway through, detailed explinations on a variery of systems, wish it went more in depth in some topics however..still an great resource to have 4/5 Link Ghost in the Wires Really intersting hacking book, opened my eyes to social engineering. RIP Kevin Mitnick 4/5 Link Building a Second Brain TBD 3.5/5 Link ","permalink":"http://localhost:1313/posts/books/","summary":"Currently Reading\u0026hellip; Title Link Writing a Compiler in Go Link Acing the System Design Interview Link Have Read Title Thoughts Rating (out of 5) Link Writing An Intreptor In Go Really good book, and introduction to Go aswell as Programing languages, love how its written with a TDD approach 5/5 Link The Missing Read Me Awesome introduction to software engineering profession. Especialy enjoyed the chapers on SEMVER and comunicating with you manager 5/5 Link The Phoenix Project Classic.","title":"Book Recomendations"},{"content":"What is Kafka? Kafka is an event streaming platform used to collect and process data streams at scale, it was Initially developed at LinkedIn in 2011. It is now open-sourced and part of the Apache Software Foundation. It is a JVM application written in Java and Scala.\nNotable Companies Using Kafka:\nLinkedIn: Kafka originated at LinkedIn. They use it for tracking operational metrics, monitoring, and event sourcing.\nNetflix: They use Kafka for real-time monitoring and event processing.\nUber: Uses Kafka for gathering metrics from its many services spread across its transportation platform.\nSpotify: Employs Kafka for tracking user activity and updating playlists in real-time.\nEvents Simply put an event is \u0026ldquo;a thing that happened\u0026rdquo;, it is a combination of Notification and State. Notable examples include\nInternet of Things (If my thermostat reads 80)\nUser Interaction (User hovers over a black dress)\nMicroservice output\nBusiness process change\nKafa data model for events is a key-value pair\nEvent Key: \u0026ldquo;20393\u0026rdquo; (Representing a user ID)\nUsually represented as a string or integer Event Value: \u0026ldquo;Hovered over a black dress for 3 seconds\u0026rdquo;\nUsually represented as one of JSON, Avro, Protocol Buffers *Note that I use the terms event and message interchangeably\nTopic Now we need a way to store all these events\u0026hellip;You can think of topics as named containers for similar events. They are really just append-only log files that contain events. All events must be sent to a topic, for example, we can create a topic named \u0026ldquo;user_metadata\u0026rdquo;, and send our event to it. Important things to note about topics are\nThey are immutable, they cannot be deleted or destroyed\nTopics are durable, retention period is configurable\nThey can only seek by offset, not indexed (this is what makes Kafka so fast!)\nProducer A Producer is a client application that publishes messages to Kafka. A producer will set the Key and Value of an event and send to a specific topic.\nConsumer A Consumer is also a client application that reads and processes events from Kafka. A Consumer must subscribe to a topic in order to receive that topic\u0026rsquo;s events. There can be many consumers for a single topic\nBroker A Broker refers to a single Kafka server instance in a Kafka cluster. The broker is responsible for receiving data (writes) from producers, storing this data, and serving it to consumers for reads. The Broker also manages the offset (or position) for consumer groups\nThis is all we need to know to get started, however, I would like to quickly mention a couple of other core Kafka components that we will not get to in this project:\nPartitions: Segments within a Kafka topic that allow data to be distributed and processed in parallel.\nReplicas: Copies of a Kafka partition that provide fault tolerance and data redundancy, ensuring data availability even if a server fails.\nConsumer groups: A set of consumers working together to consume and process data from one or multiple topics, ensuring each message is processed once and only once by one member of the group.\nKraft: A way for Kafka to manage its internal organization without relying on an external helper, making it simpler and more self-contained.\nProject Time Today we will create a Go project, simulating a real use case for Kafka. Real-time fraud detection, to get started make sure you have Go installed. Then create a directory where you want your project to live.\nmkdir kafka-fraud-detection Here we will be running Kafka on a Docker instance that can easily be spun up. Use the gist below as the content for your docker-compose.yaml file.\n%[https://gist.github.com/frankie-mur/d1591e3f347a76998de9b1c7d3293d7f]\ndocker-compose up Project Structure Great now we have Kafka running in a docker container and ports forwarded to our local host this will allow us to connect to Kafka, let\u0026rsquo;s set up the project structure we want to create a couple of folders\nmkdir -p cmd/producer cmd/consumer pkg/models pkg/bankaccount Finally, we want to initialize Go modules and install external packages. We will be using sarama it to establish a connection to our Kafka broker and faker to generate bank account creation events.\ngo mod init kafka-fraud-detection go get github.com/IBM/sarama github.com/ github.com/bxcodec/faker/v4 Create our Models and Faker Data Let\u0026rsquo;s start by setting up our BankAccount model and associated function to generate fake bank account events let\u0026rsquo;s create a file pkg/models/models.go\npackage models type BankAccount struct { FirstName string `faker:\u0026#34;first_name\u0026#34;` LastName string `faker:\u0026#34;last_name\u0026#34;` CreditCardNumber string `faker:\u0026#34;cc_number\u0026#34;` Amount float64 `faker:\u0026#34;amount\u0026#34;` } Great, This is simulating Bank Account data that a consumer will need to verify. Now let\u0026rsquo;s create that function in /pkg/bankaccount/BankAccount.go\npackage bankaccount import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/bxcodec/faker/v4\u0026#34; \u0026#34;github.com/frankie-mur/go-kafka/pkg/models\u0026#34; ) func GenerateBankAccount() (*models.BankAccount, error) { bankAccount := models.BankAccount{} err := faker.FakeData(\u0026amp;bankAccount) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to generate bank account: %w\u0026#34;, err) } return \u0026amp;bankAccount, nil } This function uses our annotated BankAccount struct and the faker library to generate fake data and return that data.\nSetting up our Producer Now that we have all that setup let\u0026rsquo;s get to the fun part, setting up our Producers and Consumers. We will start with our Producer by creating a new file in cmd/producer/producer.go and creating the function\nconst ( KafkaServerAddress = \u0026#34;localhost:9092\u0026#34; KafkaTopic = \u0026#34;bankaccount\u0026#34; ) func setupProducer() (sarama.SyncProducer, error) { config := sarama.NewConfig() config.Producer.Return.Successes = true producer, err := sarama.NewSyncProducer([]string{KafkaServerAddress}, config) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to setup producer: %w\u0026#34;, err) } return producer, nil } Let\u0026rsquo;s break down this code a bit\nDefine a function setupProducer that creates and configures a Kafka producer client. This function will return a SyncProducer if it succeeds or an error if fails\nCreate a sarama.Config struct to hold producer configuration set Producer.Return.Successes to true so the producer will return success metadata for each message.\nCall sarama.NewSyncProducer to create a new synchronous Kafka producer, passing the broker address and config. It returns the producer instance and any error from producer creation.\nSimple enough, now let\u0026rsquo;s create our main function that will call the created function above\nfunc main() { p, err := setupProducer() if err != nil { log.Fatalf(\u0026#34;Failed to setup producer: %v\u0026#34;, err.Error()) } defer p.Close() } Create our Kafka producer and assign it to a variable p\nHandle any errors creating the producer\nDefer closing the producer p connection unitl the function exits\nNow let\u0026rsquo;s create our message-sending function! Now we have a producer and can send messages let\u0026rsquo;s abstract that a bit and create another function that handles sending messages\nfunc sendKafkaMessage(producer sarama.SyncProducer, topic string, message []byte) error { msg := \u0026amp;sarama.ProducerMessage{ Topic: topic, Value: sarama.ByteEncoder(message), } _, _, err := producer.SendMessage(msg) return err } Define a function sendKafkaMessage that sends a message to Kafka using a provided producer. It accepts the producer, topic name, and message payload as parameters.\nCreate a sarama.ProducerMessage struct with the topic name and message payload.\nuse the producer.SendMessage method to send the message to Kafka asynchronously.\nNote: As you can see we only send a value, what about the key? In Kafka sending keys is optional. In our use case, we will omit the key.\nReturn any error from the send operation.\nNow back to our main function to add some code\u0026hellip;.\nfunc main() { p, err := setupProducer() if err != nil { log.Fatalf(\u0026#34;Failed to setup producer: %v\u0026#34;, err.Error()) } defer p.Close() size := 10 for i := 0; i \u0026lt; size; i++ { //Generate fake bank account data bank, err := bankaccount.GenerateBankAccount() if err != nil { fmt.Printf(\u0026#34;Failed to generate bank account: %v\u0026#34;, err.Error()) } //Convert the bank account data to []byte data, err := json.Marshal(bank) if err != nil { fmt.Printf(\u0026#34;Failed to marshal bank account: %v\u0026#34;, err.Error()) } //Send the message err = sendKafkaMessage(p, KafkaTopic, data) if err != nil { fmt.Printf(\u0026#34;Failed to send message: %v\u0026#34;, err.Error()) } } } Configure a size of 10, to send 10 messages (or events) to Kafka\nGenerate random fake bank account data using bankaccount.GenerateBankAccount() function we created\nWe must convert our bank data to []byte to send as a message in kafka so we use json.Marshal(bank) to do that for us (basically just converting our struct to stringified json)\nFinally, we send the message and check for error\nDone! Now that we can produce events lets create a Consumer to consume them!\nSetting up our Consumer Similar to our producer.go we will start with writing a function to connect as a consumer to Kafka. Lets create a new file in cmd/consumer/consumer.go\nconst ( KafkaServerAddress = \u0026#34;localhost:9092\u0026#34; KafkaTopic = \u0026#34;bankaccount\u0026#34; ) func connectConsumer() (sarama.Consumer, error) { config := sarama.NewConfig() config.Consumer.Return.Errors = true config.Consumer.Offsets.Initial = sarama.OffsetNewest // Create new consumer conn, err := sarama.NewConsumer([]string{KafkaServerAddress}, config) if err != nil { return nil, err } return conn, nil } This code:\nDefines constants for the Kafka server address and topic name.\nDefines a connectConsumer function to create a Kafka consumer client.\nCreates a sarama.Config with settings:\nConsumer.Return.Errors set to true will return any errors that occurred during the consumption of an event\nConsumer.Offsets.Initial set to sarama.OffsetNewest, this will set the newly created Consumers offset to be the newest. This will make sure the Consumer only consumes events that happened during or after its creation.\nCalls sarama.NewConsumer to create the client, passing the broker address and config.\nReturns the new consumer instance and any error.\nGreat, now let\u0026rsquo;s create our main function\nfunc main() { worker, err := connectConsumer() if err != nil { panic(err) } // Calling ConsumePartition. It will open one connection per broker // and share it for all partitions that live on it. consumer, err := worker.ConsumePartition(KafkaTopic, 0, sarama.OffsetNewest) if err != nil { panic(err) } fmt.Println(\u0026#34;Consumer started \u0026#34;) sigchan := make(chan os.Signal, 1) signal.Notify(sigchan, syscall.SIGINT, syscall.SIGTERM) // Get signal for finish doneCh := make(chan struct{}) } Some things to point out here:\nwe call worker.ConsumePartition() to start consuming messages from partition 0 of the KafkaTopic, starting from the newest offset.\nwe create sig chan channel to receive OS signals like Ctrl+C. To gracefully shutdown or log errors\nNow we need to handle when a consumer receives events\nfunc main() { ... //spins up a goroutine to fetch messages from Kafka //and handle them, while also monitoring for shutdown signals go func() { for { select { case err := \u0026lt;-consumer.Errors(): fmt.Println(err) case msg := \u0026lt;-consumer.Messages(): //Handle the message received from Kafka err := handleMessage(msg) if err != nil { fmt.Printf(\u0026#34;Failed to handle message for consumer with key: %v, with error: %v\\n\u0026#34;, msg.Key, err) } case \u0026lt;-sigchan: fmt.Println(\u0026#34;Interrupt is detected\u0026#34;) doneCh \u0026lt;- struct{}{} } } }() \u0026lt;-doneCh fmt.Println(\u0026#34;Processed all messages, awaiting more...\u0026#34;) if err := worker.Close(); err != nil { panic(err) } } without going too much into goroutines when our consumer receives an Error or Message it will be caught in our select statement, let\u0026rsquo;s define our `handleMessage` function\u0026hellip;\nfunc handleMessage(msg *sarama.ConsumerMessage) error { fmt.Printf(\u0026#34;Received message | Topic(%s) | Message(%s) \\n\u0026#34;, msg.Topic, string(msg.Value)) //Parse the message back into our BankAccount struct bankAccount := models.BankAccount{} err := json.Unmarshal(msg.Value, \u0026amp;bankAccount) if err != nil { return fmt.Errorf(\u0026#34;failed to parse message: %w\u0026#34;, err) } //Call our fake Veritas API succeeded := callVeritas(bankAccount) if !succeeded { fmt.Printf(\u0026#34;--UNAUTHORIZED-- bank account for user %s\\n\u0026#34;, bankAccount.FirstName) } else { fmt.Printf(\u0026#34;--AUTHORIZED-- bank account for user %s\\n\u0026#34;, bankAccount.FirstName) } return nil } // Simulate calling an actual verification service // randomly return true or false func callVeritas(bankAccount models.BankAccount) bool { fmt.Printf(\u0026#34;Calling Veritas for %s bank...\\n\u0026#34;, bankAccount.FirstName) return rand.Intn(2) == 0 } Notice we Unmarshal msg.Value (Remember these are key-value pairs!) to our BankAccount struct. Call our fake Authorization API that we call Vertias, and print out the results. Finished! We now have both our Producer and our Consumer, let\u0026rsquo;s use them. Let\u0026rsquo;s start with running our consumer, go to the root of the project and run the command in your terminal\nOur consumer is now running and listening for events of its subscribed topic\u0026hellip;in our case the topic bankaccount.\nNow let\u0026rsquo;s send some events! Open a new terminal (or split) and once again go to the root of the project and run the command\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1698274350519/d850de71-9c50-4bda-b54b-014c11a0b030.png align=\u0026ldquo;center\u0026rdquo;)\nDid you see that?! If you were watching the Consumer terminal you would see something similar to this\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1698274401523/071a5f52-8626-4877-ad4b-fb6dd6317496.png align=\u0026ldquo;center\u0026rdquo;)\nWe did it! If you were following along here is a quick overview of the entire flow\nOur producer sent an event to Kafka with the topic bankaccount and the value {\u0026quot;FirstName\u0026quot;:\u0026quot;Jimmie\u0026quot;,\u0026quot;LastName\u0026quot;:\u0026quot;Swaniawski\u0026quot;,\u0026quot;CreditCardNumber\u0026quot;:\u0026quot;3558018114425614\u0026quot;,\u0026quot;Amount\u0026quot;:644.78}\nOur consumer is subscribed to the topic bankaccount, therefore it receives our message\nThe message is Unmarshalled and sent to our (fake) Authorization service Veritas.\nIt is Verified (or in our case Unauthorized) and the result is printed to the console\nsource code: https://github.com/frankie-mur/go-kafka-fraud-detection/tree/main\n","permalink":"http://localhost:1313/posts/go_kafka_article/","summary":"What is Kafka? Kafka is an event streaming platform used to collect and process data streams at scale, it was Initially developed at LinkedIn in 2011. It is now open-sourced and part of the Apache Software Foundation. It is a JVM application written in Java and Scala.\nNotable Companies Using Kafka:\nLinkedIn: Kafka originated at LinkedIn. They use it for tracking operational metrics, monitoring, and event sourcing.\nNetflix: They use Kafka for real-time monitoring and event processing.","title":"Go_kafka_article"},{"content":"GopherCon 2024 Notes Talks Day 1 Charm CLi (Habbit Tracker) - Donia Chaiehloudj Charm is a teminal UI written in go uses ELM archetecure - Model, View, Update grpC backend? (To store habbits etc) check out book Learning Go with pocket sized projects TLDR;Pretty cool, have used before not very succesfully, if I work with the Architecture of the project, I think it will be a lot easier to work with. Adding \u0026ldquo;four\u0026rdquo; loop to Go compiler - Riley Thompson Add \u0026ldquo;four\u0026rdquo; and \u0026ldquo;unless\u0026rdquo; statements First Stage of Compiler Lexical Analysis and Parsing source code Scaned and tokenized Recusrive Decent Tree Add the new tokens as struct, embed the stmt struct Type Checking IR (Intermediate Representation) Contruction (noding) Converting to AST (Abstract Syntax Tree) Escape analysis (what is this) Walking through the AST \u0026ldquo;Order\u0026rdquo; step AST is converted to an IR in Static Single Assignment form IR is converted to SSA form This can be implemented in different stages in the compiler TLDR; Good talk, reminded me alot of the book Making an interpreter in go seems to be very applicable Advanced Generics Patterns - Alex Wagner Generics in Go are a hot topic (Are they event used? I have never used them) They are objectively limited in Go, also a culutural limitation Generics allow us to define types with Type arguments type Slice[E Any] []E func (s Slice[E]) Append(e E) Slice[E] Have to intantiate a type explicitly and fully when using it constraints hard to call methods on certain types Can use union types, call methods, or have the function needed as a function argument (go has union types?) Unmarshal seems like a good case for generics, however it is not a good fit for the Go implementation of generics, like json.Unmarshal TLDR; Experiment with Generics and see how far you can get with them, they have lots of contraints and you need to work around these contstraints. Building a Deterministic Interpreter in Go: Readability vs Performance - Jae Kwon a Smart Contract is a program that is run on the blockchain, it is usually untrusted code Devoped GnoVM (Go Virtual Machine) to solve issues with smart contracts and WASM Variables declared as interface always creates a pointer Scope != Allocation (for loop vars) Go reflection is limited Lightning Talks Standardizing Errors in Go: A Practical Guide with Dapr | Cassie Coyle, Diagrid Richer Error Model: code, message, details Errors messages should provide as much information as possible, evenn resources to help fix the problem Dapr provides an error package that can be used to create errors with rich details Chan Chan Chan T: A Generic Tale | Branden J. Brown finding a match in communication, talking to other goroutines communication deduplicates state Go\u0026rsquo;s use in Competitive Programming CP is answering questions via code usually taken in STDIN and STDOUT Pros: \u0026hellip;? Its fast? Cons: Compiler support, problems arent suitable to parallelizing, stdlib lacks in DSA, verbose CPP is anti-software engineering (its throw-away code), it is not a good fit for Go 7 Surprising Features of the Go Playground | Matt Dale, MongoDB support for pulling in modules allows to create multiple files (go.mod) ---filepath--- can write tests in the playground clear the terminal by prinitng fmt.Println(\u0026quot;\\x1b[2J\\x1b[H\u0026quot;) display images goplay.tool goplay.space Saving African Savannah Wildlife with Go-MQTT - Marvin Hosea Decided to use Go for the backend, because it is a good language for IoT MQTT is a lightweight pub/sub protocol, used for IoT -\u0026gt; use the Mosquitto broker (free!) This Go MQTT client library was not very good, hand bugs and not very well tested, speaker decided to work on it himself The end\u0026hellip; TLDR; Short speach, but a good talk for a even better cause. Cool how Go can be used to solve real world problems, IOT is a big one. Building a promgrammable firewall with Go - Mason Johnson Firewalls are a very important part of any network, they are used to prevent unwanted traffic Using go packages (netlink, nftables, firewall_toolkit) Get netlink connection using netlink package Add a table using the netlink connection (need to call .Flush()) Add a chain to the table and Flush create a new rule using firewall_toolkit lib Can create a set to store the mutilple IP Can connect via Kafa (sends blocked IP\u0026rsquo;s) and send the blocked IP\u0026rsquo;s to the firewall updating the blocked IP\u0026rsquo;s set TLDR; Go is great for Network programing, can program a firewall with an ergonomic API Automating Efficiency Improvement by Profile Guided Optimization - Jin Lee Uber has over 26000 Go microservices PGO is a compiler optimization technique that can be used to improve performance of Go programs Looks like a pretty complex process\u0026hellip; Go-json PGO inlining saw improvments of 12% in performance PGO originally exptended compile times by 4x, until PGO pre-processing was introduced TLDR; PGO is a great tool to improve performance of Go programs, but it is a complex process. Prototyping a Go, GraphQL, gRPC Microservice - Sadie Freeman Should always start with a prototype, articulate the problem what stack, why, etc Documents the process (pros-cons) Use existing libs/frameworks Use useful logs (!) Don\u0026rsquo;t overcomplicate, make assumptions Archetcture - inlcudes two services one for creating Alpacha and another for analytics each with their own DB Why GraphQL? More flexible querying, front-end friendly Fewer server requets Powerful pagination Define proto file schema and generate code (resolver) gRPC (service to service communication) Fast \u0026amp; Efficent HTTP/2 Protocol Buffers (Binary serialization) Clear Proto schmea Can quickly get up and running with this stack TLDR; Cool talk, very practical and useful, would like looking for into in with GraphQL, gRPC, and Buf Closing Talk (Day 1): A Decade of Debugging - Derek Parker Go debuggers were immature, did not track goroutines properly in 2014 Delve was created to address this issue by Derek Q? Who uses this? Neovimers? \u0026hellip;No everyone! It is used behind the scenes for all these debuggers TLDR; mostly a talk about the delve debugger and change log throught out the 10 years\u0026hellip;kinda cool debuggers are interesting Day 2 Who Tests the Tests? - Daniela Petruzalek A way to validate tests - Rotation Testing Why do we write tests? Tests are our insurence policy The code does what its supposed to do, happy path :) The code doesnt do what its not supposed to do, sad path :( Tests are also design tools First clients of our code Strong correlation between code that is easy to test and code that is easy to read/maintain Measauring tests Code coverage Test coverage Goodharts Law - A metric that becomes a goal stops being a good metric Test coverage is easily faked You can test tests via mutations, esentially mutate the code you are testing and it should fail. This is mutant tests coverage. In Go you can use an access the AST to program these mutations to happen during runtime Write tests for the right reasons, choose your test inputs wisely, mutation testing in go is not mature yet TLDR; very interesting talk, first time hearing of Rotation/Mutation testing, however seems like alot of work to get running Interface Internals - Keith Randall Multiply simply asks the machine to multiply - nothing needed by Go compiler For interfaces there needs to be work\u0026hellip;find the contained type..get list of methods..etc Can be done in 3 machine code instructions Interface variables bridge the gap between static and dynamic worlds, it can be reassigned Interfaces in Go are just 2 word structs with a pointer to the interface type and a pointer to the interface data Utilize an interface tab on interface data pointer to store addiotnal data -\u0026gt; all methods of an interface (sorted order) fun[0] TLDR; Very interesting talk, very useful to know how interfaces are implemented in Go and how the comiler converts to machine code Building a High-Performance Concurrent Map in Go - YunHao Zhang Go\u0026rsquo;s built in map is not concurrent safe sync.Map is provided by stdlib using built in map with a sync.RWMutex (together in a struct?) sync.Map is optimized for cahce contention - only fast to reads For RWMap there is a bottle neck for writes (blocks all reads) We can create a shared map = multiple read-write mutex maps Sounds very similar to databse sharding but for maps, writes will only block the map it is writing too Hash function to know which map to write/read to In all cases this implementation is faster, however takes more memory Can create a sub-map each new key-value pair creates a new node and all nodes are connected via pointers (LinkedList) SkipList = multiple linked lists SkipMap is 20x faster in typical cases however it is O(logn) on read/writes, sync.Map can be faster in low-currency cases TLDR; cool talk about data structures and system design, kina got lost towards the end but very interesting The Go Cryptography State of the Union - Fillipo Valsorda Post Quantum Cryptography is about the future, these computers are powerful and can crack these cryptographic algorithms Hyribs of new and old implementations to solve both problems New lib crypto/internal/mlkem768 300 lines made for readability fast and low memory footprint cryoto/tls updated defaults in 1.24 encrypted client hello in 1.23 math/rand should not be used for security before version 1.22 ChaCha8Rand is now uses in rand packages to make secure still should use crypto/rand these apis shold no longer return erros as they are now deterministic ssh package is catching up cool new API secret.Do that takes a closure and code is cleared from stack after done TLDR; cryptography is hard, Go maintainers are up for the challenge and the libs are battle tested and improving Advanced Code Instrumentation Techniques for High-Performance Trading Systems - Holly Casaletto \u0026ldquo;You can observe a system, but be prepared for it to act like it knows your watchinig!\u0026rdquo; A trading needs to be highly durable and performant (high throughout - low latency - concurrent) Merics API where we need to handle locking, etc TLDR; was kinda tired for this one, was very verbose and a bit too much info for me Digital Audio from Scratch - Patrick Stephen Finally some music using a lib daw we are sending bytes into speaker -\u0026gt; higher byte = higher volume, pattern = frequency speaker is going through different waves/numbers/and math to get a cool sound TLDR; Music is cool and interesting (also complicated), you can do it in Go if you want.. Lightning Talks How to Mock Your Coworkers - Dylan Bourque from CrowdStrike Mocking is test doubling - you can mock out the dependencies of a function Don\u0026rsquo;t export your mocks\u0026hellip;write(or generate) your mocks internal to your project \u0026hellip;dont actaully mock your coworkers lol Lightning-Fast Database Tests - Robin Hallabro-Kokko DB tests are usually the slowest tests Fast tests lower the barier for creating new tests (CI/CD, faster development) pgtestdb creates emphemal postgres databases quickly and handles lifecycle it is very quick and seems easy to use Built it! With or WIthout Tools - Matthew Sanabria Speaker compares refactoring production code to building his new kitchen Right Tools, Wrong Knowledge - Learn! Read the docs Wrong Tools, Write Tests - It happends, verify tools work by writing tests Know when to seek an expert, and observe their work Greatest tool in your toolbox is you You can store that in a container registery? - Siva Manivannan Yes, you can store things that are not containers in a container registry Container Registry is metadata and a blob If you implment these ~20 API endpoints you have a container registry Would You Like a GUI With That? - Andy Joseph, ProntoGUI ProntoGUI is a lib that allows to build GUI\u0026rsquo;s in Go Events from frontend to backend are via gRPC Kinda cool? Don\u0026rsquo;t see how its better/worse than other GUI libs Embracing the Replace: Our Journey to Fixing the Plugin System in HashiCorp Packer - Wilken Rivera, HashiCorp Company had a breaking change bc they were using a experimental package that eventaully was dropped Options were limited, decided to fork the package and fix it added a CLI command to fix the issue for their users when having a breaking change, stop the bleed, and do whats best for users even if its the hardest A 10-Year Retrospective on Building an Infrastructure Start-up in Go - Alan Shreve ngrok originall use was to have public url for local project running on a port when started in 2014 go was still very immature in problems related to ngrok sqlc for database stuff Use lots of gRPC over ~100 services Interceptors (like middleware but for gRPC) can be used on client and server side use RichErrors (custom Error type struct) and have a client side interceptor that catches this Replicate all the things Integration Testing - matrix of thosands of tests run in production 24/7 in a loop (wow) Define your errors to help users ngrok\u0026rsquo;s success is acredited to Go and its proerties dont get attached to code software is meant to be rewritten Processing Millions of Events Per Second Reliably Using Generics - Alan Braithwaite Stream processing processes a continouts stream of distint events consuming from some source external to the system and delivering to some desination external to the system Goals for the project, be fast, plugins are easy to create, support any serializable type, batteries included, at least one delivery Okay this is where generics comes in, using the Go lib kawa the types are used as generics Speaker argues against using channels due to their overhead and unseen complexity TLDR; stream proccessing is very interesting, i want to look into this. Project seems cool Building a Self-Regulating Pressure Relief Mechanism in Go - Ellen Gao it is importnat for systems to have healthy utlization (cpu?) Not always best option to just increase resources, (cost, complexity) A presure valave can detect a soft limit and perform action slow down processing of non-critical processes, these messges are processed later Program uses a messague broker pub/sub system to mimic opearations and priority Basic throttler implemented with time.Sleep program now completes Adds 3 different throttling strategies (block, fixed delay, and backoff) for each priority via interfaces Then implements recovery, most was already implemented so not much changes needed TLDR; interesting talk, good speaker, learned some about throttling and pressure relief, liked the the flow from talk to code. Go in the Smallest of Places - Patricio Whittingslow] First major software was the Apollo spaceship and Margarette Lewis was first SWE, written in assembly C was created a higher level language (comparatievly) for micro controllers TinyGo makes Go usuable for MCU development Working with MCU\u0026rsquo;s is verbose, you needd to shift a bunch of bits in memory (registers) TLDR; TinyGo is awesome and I need to use it with the programabale badge I have Range Over Function Types - Ian Lance Taylor New feature in 1.23 release Ways to loop over all of an elements in a Set in Go Extend the for range, or add iterator type, how about both .All() method should be created for container types which returns an iterator TLDR; finally iterators in go, altough don\u0026rsquo;t love the implementation End of Day 2 ","permalink":"http://localhost:1313/posts/gophercon/","summary":"GopherCon 2024 Notes Talks Day 1 Charm CLi (Habbit Tracker) - Donia Chaiehloudj Charm is a teminal UI written in go uses ELM archetecure - Model, View, Update grpC backend? (To store habbits etc) check out book Learning Go with pocket sized projects TLDR;Pretty cool, have used before not very succesfully, if I work with the Architecture of the project, I think it will be a lot easier to work with.","title":"Gophercon 2024 Note Dump"},{"content":"This is my first Hugo site it seems pretty cool\u0026hellip;. I have sest up PaperMod theme and menu bar along with tags and a fuzzy search. I hear Hugo is written in Go, Im interested in learning more about it (possibly Contribute to it?) anyways looking forward to making more posts. Kaizen!\n","permalink":"http://localhost:1313/posts/my-first-post/","summary":"This is my first Hugo site it seems pretty cool\u0026hellip;. I have sest up PaperMod theme and menu bar along with tags and a fuzzy search. I hear Hugo is written in Go, Im interested in learning more about it (possibly Contribute to it?) anyways looking forward to making more posts. Kaizen!","title":"My First Post"}]