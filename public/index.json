[{"content":"Over the past week I spent some time learning Kubernetes. I\u0026rsquo;m going to share some of the things I learned and some of the things I\u0026rsquo;m still learning.\nKubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized application\nHistory Way back when\u0026hellip;.there were \u0026ldquo;Traditional Deployments\u0026rdquo; 2000\u0026rsquo;s On Premises, Bare metal hardware Teams of sysadmins handle provisioning and managing of fleets of servers Monoliths\u0026hellip;.. Custom, home grown monitoring and tooling 2010s VM\u0026rsquo;s starting to gain massive adoption Clouds services enables VM\u0026rsquo;s to be created and destroyed Configuration management tolling, and overall imporived tooling 2020s Containers, containers, containers Workload orchestrator\u0026rsquo;s (like Kubernetes!) enable treating clusters of machines as a single resource These orchestrators included utilities and interfaces to solve many challenges Resources Namespace Provide a mechanism to group resources within a cluster There a 4 initial namespaces: default, kube-system, kube-node-lease, kube-public By default, they do not act as network or secutrity boundary Kubernetes is the conductor of a container orchestra Key Features\nService Discovery/Load Balancing Storage Orchestration Automate Rollouts/Rollbacks Self-healing Secret and Configuration Management Horizontal Scaling A Kubernetes cluster consists of two types of resources:\nThe Control Plane coordinates the cluster Nodes are the workers that run applications Control Plane The control plane is responsible for managing the xluster. THe control Plane coordinates all activities in the cluster such as: Scheduling applications Maintaining applications desired state Scaling applications rolling out new updates Nodes A node is a VM or a physical computer that serves as a worker machine in a Kubernetes cluster Each node has a Kublet, an agent for managing the node and communicating with the control plane Pods The \u0026ldquo;smallest\u0026rdquo; deployable unit Pod can contain multiple containers Init containers Sidecar containers Pod containers share the same Network namespace (share IP/port) Have same loopback network interface (localhost) Ports can be reused within same node (as long as separate pod) You will almost never create a pod directly (should be done via Files) simple pod yaml file apiVersion: v1 kind: Pod metadata: name: my-nginx labels: app: nginx rel: stable spec: containers: - name: my-nginx image: nginx:alpine ports: - containerPort: 80 resources: {} Probes A probe is a diagnostic performed periodically by the kublet on a container Types of probes: Liveness, can be used to determine if a Pod is healthy and running as expected When should a container restart? Readiness probes can be used to determine if a pod is ready to accept requests When should a container start receiving traffic? These should be set during .yaml file creation Actions you can perform on probes ExecAction - Executes an action inside the container TCPSocketAction - TCP check against the containers IP address on a specified port HTTPGetAction - HTTP GET request against a container Probes can have the following results Success Failure Unknown simple pod file with probes apiVersion: v1 kind: Pod metadata: name: my-nginx labels: app: nginx rel: stable spec: containers: - name: my-nginx image: nginx:alpine resources: limits: memory: \u0026#34;128Mi\u0026#34; #128 MB cpu: \u0026#34;200m\u0026#34; #200 millicpu (.2 cpu or 20% of the cpu) ports: - containerPort: 80 livenessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 15 timeoutSeconds: 2 # Default is 1 periodSeconds: 5 # Default is 10 failureThreshold: 1 # Default is 3 readinessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 3 periodSeconds: 5 # Default is 10 failureThreshold: 1 # Default is 3 Replica Set A ReplicaSet ensures that a specified number of pod replicas are running at any given time. If a pod crashes or is deleted, the ReplicaSet automatically creates a new pod to replace it, maintaining the desired number of replicas. ReplicaSets act a a Pod controller Self healing mechanism Ensure the requested number of Pods are available Provide fault-tolerance Can be used to scale Pods Relies on a Pod template You will almost never create a ReplicaSet directly Labels are the link between ReplicaSets and Pods Deployments A Deployment provides a declarative way to preform updates to applications. It manages the creation and scaling of ReplicaSets, Deployments are a higher-level concept that includes many additional features beyond what ReplicaSets offer. If you are deploying stateless applications on Kubernetes, a Deployment is generally the resource type you should use. A Deployment ends up deploying a ReplicaSet but adds the following functionality: Declarative Updates: Declaratively update your applications. You specify the desired state in the Deployment configuration, and the Deployment controller will ensure that the current state matches the desired state. Rolling Updates: update the pods in a controlled manner with zero downtime. This allows you to update your application to a new version while keeping the old version running until the new version is ready. Rollback: If an update fails, you can easily roll back to a previous revision of the Deployment. Kubernetes keeps track of Deployment revisions for easy rollback. A deployment manages Pods, essentially a wrapper over ReplicaSet: Pods are managed using ReplicaSets Scales ReplicaSets, which scale Pods Supports zero-downtime updates by creating and destroying ReplicaSets Creates a unique label that is assigned to the ReplicaSet and generate Pods YAML is very similar to a ReplicaSet simple deployment file apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx labels: app: my-nginx spec: replicas: 2 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - name: my-nginx image: nginx:alpine ports: - containerPort: 80 resources: limits: memory: \u0026#34;128Mi\u0026#34; #128 MB cpu: \u0026#34;200m\u0026#34; #200 millicpu (.2 cpu or 20% of the cpu) note: this will also create ReplicaSet\nServices Each pod is assigned an IP address to make it reachable via the network, but the pods are considered ephemeral and may be deleted at any time. To provide a stable way to address a set of pods (e.g. from a Deployment) we use a Service. There are a variety of kinds of services that provide access to pods from within or outside of the cluster. A service provides a single point of entry for accessing one or more pods Since pods live and die, can we rely on Pod IP Addresses?\nNo! That where services come in Abstract Pod IPS addresses from consumers Load balances between pods Relies on labels to assosiate a service with a pod Nodes kube-proxy creates a virtual IP for services Layer 4 (TCP/UDP over IP) Services are not ephemeral Service Types ClusterIP (default): Exposes the Service on a cluster-internal IP. Only accessible from within the cluster. NodePort: Exposes the Service on each Node’s IP at a static port. External traffic can reach the service via \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;. LoadBalancer: Exposes the Service externally using a cloud provider\u0026rsquo;s load balancer. ExternalName: Maps the Service to a DNS name, useful for external services outside the cluster. Service .yaml file example apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer spec: type: LoadBalancer selector: app: my-nginx ports: - name: \u0026#34;80\u0026#34; port: 80 targetPort: 80 Job In Kubernetes, a Job creates one or more Pods and ensures that a specified number of them complete successfully. Jobs are used for short-lived, one-time tasks such as batch processing and other short-lived operations. Example .yaml apiVersion: batch/v1 kind: Job metadata: name: echo-date-better namespace: 04--job spec: parallelism: 2 completions: 2 activeDeadlineSeconds: 100 backoffLimit: 1 template: metadata: labels: app: echo-date spec: containers: - name: echo image: busybox:1.36.1 command: [\u0026#34;date\u0026#34;] restartPolicy: Never Comes with properties to set parallelism, completions, active deadlines, and backoffLmits CronJob Adds the concept of a \u0026ldquo;schedule\u0026rdquo; to jobs Used for periodic execution of workloads that run to completion This will essentially create a Job that runs on specified schedule, a regualr Job will only run at creation DaemonSet A DaemonSet ensures that all (or some) nodes run a copy of a Pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. DaemonSets are typically used for deploying system-level agents and tools, such as log collectors, monitoring agents, or other utilities that should run on every node. StatefulSet In Kubernetes, a StatefulSet is used to manage stateful applications. Unlike Deployments, StatefulSets maintain a sticky identity for each of their Pods. These Pods are created from the same spec but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling. Enables configuring workloads that require state management (like primary vs read-replica databases) Storage Volumes Can be used to hold data and state for pods and containers A Pod can have multiple Volumes attached to it Containers rely on a mountPath to access a volume Volume Types emptyDir:\nA volume that is initially empty and is created when a Pod is assigned to a Node. The data in an emptyDir volume is stored on the node\u0026rsquo;s filesystem and is deleted when the Pod is removed from the node. Use Case: Temporary storage, caching, or when you need a scratch space for a Pod. hostPath:\nA volume that mounts a file or directory from the host node\u0026rsquo;s filesystem into a Pod. This allows a Pod to access specific files or directories on the host. Use Case: Accessing host-level resources, such as logs or Docker sockets. Be cautious as this can tie your Pod to a specific node and pose security risks. nfs:\nA volume that allows Pods to mount a Network File System (NFS) share. This enables Pods to read and write data on a remote NFS server. Use Case: Sharing data between multiple Pods or across nodes, persistent storage with remote access. configMap/secret:\nVolumes that allow Pods to consume configuration data or sensitive information (like passwords) as files or environment variables. ConfigMaps handle general configuration, while Secrets handle sensitive data. Two primary styles Property Like (will show as env variables within a container) File like (will be within a file in the container) Secrets are similar to config maps, however the data is base64 encoded (to support binary data, NOT a security mechanism) Secret Type Use Case: Storing configuration files, environment variables, certificates, or credentials securely. persistentVolumeClaim (PVC):\nProvides API for creating, managing, and consuming storage that lives beyone the life on an individual pod A volume type that abstracts the underlying storage resource and binds to a Persistent Volume (PV). PVCs allow you to request specific storage resources, such as size and access modes, from the cluster. Acess Modes: ReadWriteOnce (and ReadWriteOncePod) ReadOnlyMany ReadWriteMany Reclaim Policy: Retain vs Delete Use Case: Persistent storage for stateful applications, such as databases, where data needs to persist across Pod restarts or rescheduling. Cloud:\nCloud provider-specific volumes that integrate with cloud storage services, such as AWS EBS (Elastic Block Store), GCP Persistent Disk, or Azure Disk. These volumes provide managed, scalable, and persistent storage solutions. Use Case: Persistent storage in cloud environments, often used for databases, file storage, or any application requiring reliable and durable storage. Ingres Enables routing traffic to many services via single external LoadBalancer Many options to choose from, like Ingress-nginx, HAProxy, Istio\u0026hellip; Only officially supports layer 7 routing, however layer 4 support is there (GatewayAPI) Enter Helm\u0026hellip;.What is it? Helm is the de-facto standard for distributing software for Kubernetes It is a combination of: Package manager Templating engine Primary use cases: Application deployment Environment management (staging vs prod..) Commands helm install / helm upgrade helm rollback Sources https://www.youtube.com/watch?v=2T86xAtR6Fo\u0026t=12477s https://www.boot.dev/lessons/02957d87-58be-4fb5-b2b2-768170ed7860 ","permalink":"http://localhost:1313/posts/k8scrashcourse/","summary":"Over the past week I spent some time learning Kubernetes. I\u0026rsquo;m going to share some of the things I learned and some of the things I\u0026rsquo;m still learning.\nKubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized application\nHistory Way back when\u0026hellip;.there were \u0026ldquo;Traditional Deployments\u0026rdquo; 2000\u0026rsquo;s On Premises, Bare metal hardware Teams of sysadmins handle provisioning and managing of fleets of servers Monoliths\u0026hellip;.. Custom, home grown monitoring and tooling 2010s VM\u0026rsquo;s starting to gain massive adoption Clouds services enables VM\u0026rsquo;s to be created and destroyed Configuration management tolling, and overall imporived tooling 2020s Containers, containers, containers Workload orchestrator\u0026rsquo;s (like Kubernetes!","title":"K8s Crash Course"},{"content":"In its simpliest form, context is an interface from the Go docs\nPackage context defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. This may seem confusing but in essense context serves two purposes within a Go application Timeouts Storing values In general to craete a context you will call the context.Background() function. when you are unsure of what context to use you will use context.TODO() function\nTimeouts func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() val, err := doSomething(ctx) ... } In this example we are creating a context with a timeout of 1 second, this means that if the function doSomething does not return within 1 second the context will be canceled.\nStoring Values func main() { ctx := context.WithValue(context.Background(), \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) val := ctx.Value(\u0026#34;key\u0026#34;) val, err := doSomething(ctx) ... } In this example we are creating a context with a value of \u0026ldquo;key\u0026rdquo; and \u0026ldquo;value\u0026rdquo;. We can then access this value later in our code by calling ctx.Value(\u0026quot;key\u0026quot;), for example:\nfunc doSomething(ctx context.Context) (string, error) { val := ctx.Value(\u0026#34;key\u0026#34;).(string) // do something with val ... } Conclusion Context is a powerful tool in Go, it is used to pass values and timeouts between functions. It is used in many places in the Go standard library, such as the net/http package and the database/sql package. Have fun using it!\n","permalink":"http://localhost:1313/posts/go_context/","summary":"In its simpliest form, context is an interface from the Go docs\nPackage context defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. This may seem confusing but in essense context serves two purposes within a Go application Timeouts Storing values In general to craete a context you will call the context.Background() function. when you are unsure of what context to use you will use context.","title":"Go: some context about context"},{"content":"Oauth 2.0 TLDR; Oauth2 is an authorization protocol used to grant access to a resource (like a service or API) without sharing credentials. For example a spotify TUI client, would use Oauth2 to request access to the spotify API, then use that token to access the spotify API on your behalf, without having to share your credentials. This will also be limited to the scopes that you have allowed the client to access. ","permalink":"http://localhost:1313/posts/oauth_2/","summary":"Oauth 2.0 TLDR; Oauth2 is an authorization protocol used to grant access to a resource (like a service or API) without sharing credentials. For example a spotify TUI client, would use Oauth2 to request access to the spotify API, then use that token to access the spotify API on your behalf, without having to share your credentials. This will also be limited to the scopes that you have allowed the client to access.","title":"Oauth2"},{"content":"What is Kafka? Kafka is an event streaming platform used to collect and process data streams at scale, it was Initially developed at LinkedIn in 2011. It is now open-sourced and part of the Apache Software Foundation. It is a JVM application written in Java and Scala.\nNotable Companies Using Kafka:\nLinkedIn: Kafka originated at LinkedIn. They use it for tracking operational metrics, monitoring, and event sourcing.\nNetflix: They use Kafka for real-time monitoring and event processing.\nUber: Uses Kafka for gathering metrics from its many services spread across its transportation platform.\nSpotify: Employs Kafka for tracking user activity and updating playlists in real-time.\nEvents Simply put an event is \u0026ldquo;a thing that happened\u0026rdquo;, it is a combination of Notification and State. Notable examples include\nInternet of Things (If my thermostat reads 80)\nUser Interaction (User hovers over a black dress)\nMicroservice output\nBusiness process change\nKafa data model for events is a key-value pair\nEvent Key: \u0026ldquo;20393\u0026rdquo; (Representing a user ID)\nUsually represented as a string or integer Event Value: \u0026ldquo;Hovered over a black dress for 3 seconds\u0026rdquo;\nUsually represented as one of JSON, Avro, Protocol Buffers *Note that I use the terms event and message interchangeably\nTopic Now we need a way to store all these events\u0026hellip;You can think of topics as named containers for similar events. They are really just append-only log files that contain events. All events must be sent to a topic, for example, we can create a topic named \u0026ldquo;user_metadata\u0026rdquo;, and send our event to it. Important things to note about topics are\nThey are immutable, they cannot be deleted or destroyed\nTopics are durable, retention period is configurable\nThey can only seek by offset, not indexed (this is what makes Kafka so fast!)\nProducer A Producer is a client application that publishes messages to Kafka. A producer will set the Key and Value of an event and send to a specific topic.\nConsumer A Consumer is also a client application that reads and processes events from Kafka. A Consumer must subscribe to a topic in order to receive that topic\u0026rsquo;s events. There can be many consumers for a single topic\nBroker A Broker refers to a single Kafka server instance in a Kafka cluster. The broker is responsible for receiving data (writes) from producers, storing this data, and serving it to consumers for reads. The Broker also manages the offset (or position) for consumer groups\nThis is all we need to know to get started, however, I would like to quickly mention a couple of other core Kafka components that we will not get to in this project:\nPartitions: Segments within a Kafka topic that allow data to be distributed and processed in parallel.\nReplicas: Copies of a Kafka partition that provide fault tolerance and data redundancy, ensuring data availability even if a server fails.\nConsumer groups: A set of consumers working together to consume and process data from one or multiple topics, ensuring each message is processed once and only once by one member of the group.\nKraft: A way for Kafka to manage its internal organization without relying on an external helper, making it simpler and more self-contained.\nProject Time Today we will create a Go project, simulating a real use case for Kafka. Real-time fraud detection, to get started make sure you have Go installed. Then create a directory where you want your project to live.\nmkdir kafka-fraud-detection Here we will be running Kafka on a Docker instance that can easily be spun up. Use the gist below as the content for your docker-compose.yaml file.\n%[https://gist.github.com/frankie-mur/d1591e3f347a76998de9b1c7d3293d7f]\ndocker-compose up Project Structure Great now we have Kafka running in a docker container and ports forwarded to our local host this will allow us to connect to Kafka, let\u0026rsquo;s set up the project structure we want to create a couple of folders\nmkdir -p cmd/producer cmd/consumer pkg/models pkg/bankaccount Finally, we want to initialize Go modules and install external packages. We will be using sarama it to establish a connection to our Kafka broker and faker to generate bank account creation events.\ngo mod init kafka-fraud-detection go get github.com/IBM/sarama github.com/ github.com/bxcodec/faker/v4 Create our Models and Faker Data Let\u0026rsquo;s start by setting up our BankAccount model and associated function to generate fake bank account events let\u0026rsquo;s create a file pkg/models/models.go\npackage models type BankAccount struct { FirstName string `faker:\u0026#34;first_name\u0026#34;` LastName string `faker:\u0026#34;last_name\u0026#34;` CreditCardNumber string `faker:\u0026#34;cc_number\u0026#34;` Amount float64 `faker:\u0026#34;amount\u0026#34;` } Great, This is simulating Bank Account data that a consumer will need to verify. Now let\u0026rsquo;s create that function in /pkg/bankaccount/BankAccount.go\npackage bankaccount import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/bxcodec/faker/v4\u0026#34; \u0026#34;github.com/frankie-mur/go-kafka/pkg/models\u0026#34; ) func GenerateBankAccount() (*models.BankAccount, error) { bankAccount := models.BankAccount{} err := faker.FakeData(\u0026amp;bankAccount) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to generate bank account: %w\u0026#34;, err) } return \u0026amp;bankAccount, nil } This function uses our annotated BankAccount struct and the faker library to generate fake data and return that data.\nSetting up our Producer Now that we have all that setup let\u0026rsquo;s get to the fun part, setting up our Producers and Consumers. We will start with our Producer by creating a new file in cmd/producer/producer.go and creating the function\nconst ( KafkaServerAddress = \u0026#34;localhost:9092\u0026#34; KafkaTopic = \u0026#34;bankaccount\u0026#34; ) func setupProducer() (sarama.SyncProducer, error) { config := sarama.NewConfig() config.Producer.Return.Successes = true producer, err := sarama.NewSyncProducer([]string{KafkaServerAddress}, config) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to setup producer: %w\u0026#34;, err) } return producer, nil } Let\u0026rsquo;s break down this code a bit\nDefine a function setupProducer that creates and configures a Kafka producer client. This function will return a SyncProducer if it succeeds or an error if fails\nCreate a sarama.Config struct to hold producer configuration set Producer.Return.Successes to true so the producer will return success metadata for each message.\nCall sarama.NewSyncProducer to create a new synchronous Kafka producer, passing the broker address and config. It returns the producer instance and any error from producer creation.\nSimple enough, now let\u0026rsquo;s create our main function that will call the created function above\nfunc main() { p, err := setupProducer() if err != nil { log.Fatalf(\u0026#34;Failed to setup producer: %v\u0026#34;, err.Error()) } defer p.Close() } Create our Kafka producer and assign it to a variable p\nHandle any errors creating the producer\nDefer closing the producer p connection unitl the function exits\nNow let\u0026rsquo;s create our message-sending function! Now we have a producer and can send messages let\u0026rsquo;s abstract that a bit and create another function that handles sending messages\nfunc sendKafkaMessage(producer sarama.SyncProducer, topic string, message []byte) error { msg := \u0026amp;sarama.ProducerMessage{ Topic: topic, Value: sarama.ByteEncoder(message), } _, _, err := producer.SendMessage(msg) return err } Define a function sendKafkaMessage that sends a message to Kafka using a provided producer. It accepts the producer, topic name, and message payload as parameters.\nCreate a sarama.ProducerMessage struct with the topic name and message payload.\nuse the producer.SendMessage method to send the message to Kafka asynchronously.\nNote: As you can see we only send a value, what about the key? In Kafka sending keys is optional. In our use case, we will omit the key.\nReturn any error from the send operation.\nNow back to our main function to add some code\u0026hellip;.\nfunc main() { p, err := setupProducer() if err != nil { log.Fatalf(\u0026#34;Failed to setup producer: %v\u0026#34;, err.Error()) } defer p.Close() size := 10 for i := 0; i \u0026lt; size; i++ { //Generate fake bank account data bank, err := bankaccount.GenerateBankAccount() if err != nil { fmt.Printf(\u0026#34;Failed to generate bank account: %v\u0026#34;, err.Error()) } //Convert the bank account data to []byte data, err := json.Marshal(bank) if err != nil { fmt.Printf(\u0026#34;Failed to marshal bank account: %v\u0026#34;, err.Error()) } //Send the message err = sendKafkaMessage(p, KafkaTopic, data) if err != nil { fmt.Printf(\u0026#34;Failed to send message: %v\u0026#34;, err.Error()) } } } Configure a size of 10, to send 10 messages (or events) to Kafka\nGenerate random fake bank account data using bankaccount.GenerateBankAccount() function we created\nWe must convert our bank data to []byte to send as a message in kafka so we use json.Marshal(bank) to do that for us (basically just converting our struct to stringified json)\nFinally, we send the message and check for error\nDone! Now that we can produce events lets create a Consumer to consume them!\nSetting up our Consumer Similar to our producer.go we will start with writing a function to connect as a consumer to Kafka. Lets create a new file in cmd/consumer/consumer.go\nconst ( KafkaServerAddress = \u0026#34;localhost:9092\u0026#34; KafkaTopic = \u0026#34;bankaccount\u0026#34; ) func connectConsumer() (sarama.Consumer, error) { config := sarama.NewConfig() config.Consumer.Return.Errors = true config.Consumer.Offsets.Initial = sarama.OffsetNewest // Create new consumer conn, err := sarama.NewConsumer([]string{KafkaServerAddress}, config) if err != nil { return nil, err } return conn, nil } This code:\nDefines constants for the Kafka server address and topic name.\nDefines a connectConsumer function to create a Kafka consumer client.\nCreates a sarama.Config with settings:\nConsumer.Return.Errors set to true will return any errors that occurred during the consumption of an event\nConsumer.Offsets.Initial set to sarama.OffsetNewest, this will set the newly created Consumers offset to be the newest. This will make sure the Consumer only consumes events that happened during or after its creation.\nCalls sarama.NewConsumer to create the client, passing the broker address and config.\nReturns the new consumer instance and any error.\nGreat, now let\u0026rsquo;s create our main function\nfunc main() { worker, err := connectConsumer() if err != nil { panic(err) } // Calling ConsumePartition. It will open one connection per broker // and share it for all partitions that live on it. consumer, err := worker.ConsumePartition(KafkaTopic, 0, sarama.OffsetNewest) if err != nil { panic(err) } fmt.Println(\u0026#34;Consumer started \u0026#34;) sigchan := make(chan os.Signal, 1) signal.Notify(sigchan, syscall.SIGINT, syscall.SIGTERM) // Get signal for finish doneCh := make(chan struct{}) } Some things to point out here:\nwe call worker.ConsumePartition() to start consuming messages from partition 0 of the KafkaTopic, starting from the newest offset.\nwe create sig chan channel to receive OS signals like Ctrl+C. To gracefully shutdown or log errors\nNow we need to handle when a consumer receives events\nfunc main() { ... //spins up a goroutine to fetch messages from Kafka //and handle them, while also monitoring for shutdown signals go func() { for { select { case err := \u0026lt;-consumer.Errors(): fmt.Println(err) case msg := \u0026lt;-consumer.Messages(): //Handle the message received from Kafka err := handleMessage(msg) if err != nil { fmt.Printf(\u0026#34;Failed to handle message for consumer with key: %v, with error: %v\\n\u0026#34;, msg.Key, err) } case \u0026lt;-sigchan: fmt.Println(\u0026#34;Interrupt is detected\u0026#34;) doneCh \u0026lt;- struct{}{} } } }() \u0026lt;-doneCh fmt.Println(\u0026#34;Processed all messages, awaiting more...\u0026#34;) if err := worker.Close(); err != nil { panic(err) } } without going too much into goroutines when our consumer receives an Error or Message it will be caught in our select statement, let\u0026rsquo;s define our `handleMessage` function\u0026hellip;\nfunc handleMessage(msg *sarama.ConsumerMessage) error { fmt.Printf(\u0026#34;Received message | Topic(%s) | Message(%s) \\n\u0026#34;, msg.Topic, string(msg.Value)) //Parse the message back into our BankAccount struct bankAccount := models.BankAccount{} err := json.Unmarshal(msg.Value, \u0026amp;bankAccount) if err != nil { return fmt.Errorf(\u0026#34;failed to parse message: %w\u0026#34;, err) } //Call our fake Veritas API succeeded := callVeritas(bankAccount) if !succeeded { fmt.Printf(\u0026#34;--UNAUTHORIZED-- bank account for user %s\\n\u0026#34;, bankAccount.FirstName) } else { fmt.Printf(\u0026#34;--AUTHORIZED-- bank account for user %s\\n\u0026#34;, bankAccount.FirstName) } return nil } // Simulate calling an actual verification service // randomly return true or false func callVeritas(bankAccount models.BankAccount) bool { fmt.Printf(\u0026#34;Calling Veritas for %s bank...\\n\u0026#34;, bankAccount.FirstName) return rand.Intn(2) == 0 } Notice we Unmarshal msg.Value (Remember these are key-value pairs!) to our BankAccount struct. Call our fake Authorization API that we call Vertias, and print out the results. Finished! We now have both our Producer and our Consumer, let\u0026rsquo;s use them. Let\u0026rsquo;s start with running our consumer, go to the root of the project and run the command in your terminal\nOur consumer is now running and listening for events of its subscribed topic\u0026hellip;in our case the topic bankaccount.\nNow let\u0026rsquo;s send some events! Open a new terminal (or split) and once again go to the root of the project and run the command\nDid you see that?! If you were watching the Consumer terminal you would see something similar to this\nWe did it! If you were following along here is a quick overview of the entire flow\nOur producer sent an event to Kafka with the topic bankaccount and the value {\u0026quot;FirstName\u0026quot;:\u0026quot;Jimmie\u0026quot;,\u0026quot;LastName\u0026quot;:\u0026quot;Swaniawski\u0026quot;,\u0026quot;CreditCardNumber\u0026quot;:\u0026quot;3558018114425614\u0026quot;,\u0026quot;Amount\u0026quot;:644.78}\nOur consumer is subscribed to the topic bankaccount, therefore it receives our message\nThe message is Unmarshalled and sent to our (fake) Authorization service Veritas.\nIt is Verified (or in our case Unauthorized) and the result is printed to the console\nsource code: https://github.com/frankie-mur/go-kafka-fraud-detection/tree/main\n","permalink":"http://localhost:1313/posts/go_kafka_article/","summary":"What is Kafka? Kafka is an event streaming platform used to collect and process data streams at scale, it was Initially developed at LinkedIn in 2011. It is now open-sourced and part of the Apache Software Foundation. It is a JVM application written in Java and Scala.\nNotable Companies Using Kafka:\nLinkedIn: Kafka originated at LinkedIn. They use it for tracking operational metrics, monitoring, and event sourcing.\nNetflix: They use Kafka for real-time monitoring and event processing.","title":"Go with the Flow: Event Streaming with Go and Kafka"},{"content":"Currently Reading\u0026hellip; Title Link Writing a Compiler in Go Link Acing the System Design Interview Link Have Read Title Thoughts Rating (out of 5) Link Writing An Intreptor In Go Really good book, and introduction to Go aswell as Programing languages, love how its written with a TDD approach 5/5 Link The Missing Read Me Awesome introduction to software engineering profession. Especialy enjoyed the chapers on SEMVER and comunicating with you manager 5/5 Link The Phoenix Project Classic. Good to take a break from dense techonology books 4/5 Link How Linux Works The introduction to Linux and Computer Systems 4/5 Link System Design Interview Vol.2 Read about halfway through, detailed explinations on a variery of systems, wish it went more in depth in some topics however..still an great resource to have 4/5 Link Ghost in the Wires Really intersting hacking book, opened my eyes to social engineering. RIP Kevin Mitnick 4/5 Link Building a Second Brain TBD 3.5/5 Link ","permalink":"http://localhost:1313/posts/books/","summary":"Currently Reading\u0026hellip; Title Link Writing a Compiler in Go Link Acing the System Design Interview Link Have Read Title Thoughts Rating (out of 5) Link Writing An Intreptor In Go Really good book, and introduction to Go aswell as Programing languages, love how its written with a TDD approach 5/5 Link The Missing Read Me Awesome introduction to software engineering profession. Especialy enjoyed the chapers on SEMVER and comunicating with you manager 5/5 Link The Phoenix Project Classic.","title":"Book Recomendations"},{"content":"GopherCon 2024 Notes Talks Day 1 Charm CLi (Habbit Tracker) - Donia Chaiehloudj Charm is a teminal UI written in go uses ELM archetecure - Model, View, Update grpC backend? (To store habbits etc) check out book Learning Go with pocket sized projects TLDR;Pretty cool, have used before not very succesfully, if I work with the Architecture of the project, I think it will be a lot easier to work with. Adding \u0026ldquo;four\u0026rdquo; loop to Go compiler - Riley Thompson Add \u0026ldquo;four\u0026rdquo; and \u0026ldquo;unless\u0026rdquo; statements First Stage of Compiler Lexical Analysis and Parsing source code Scaned and tokenized Recusrive Decent Tree Add the new tokens as struct, embed the stmt struct Type Checking IR (Intermediate Representation) Contruction (noding) Converting to AST (Abstract Syntax Tree) Escape analysis (what is this) Walking through the AST \u0026ldquo;Order\u0026rdquo; step AST is converted to an IR in Static Single Assignment form IR is converted to SSA form This can be implemented in different stages in the compiler TLDR; Good talk, reminded me alot of the book Making an interpreter in go seems to be very applicable Advanced Generics Patterns - Alex Wagner Generics in Go are a hot topic (Are they event used? I have never used them) They are objectively limited in Go, also a culutural limitation Generics allow us to define types with Type arguments type Slice[E Any] []E func (s Slice[E]) Append(e E) Slice[E] Have to intantiate a type explicitly and fully when using it constraints hard to call methods on certain types Can use union types, call methods, or have the function needed as a function argument (go has union types?) Unmarshal seems like a good case for generics, however it is not a good fit for the Go implementation of generics, like json.Unmarshal TLDR; Experiment with Generics and see how far you can get with them, they have lots of contraints and you need to work around these contstraints. Building a Deterministic Interpreter in Go: Readability vs Performance - Jae Kwon a Smart Contract is a program that is run on the blockchain, it is usually untrusted code Devoped GnoVM (Go Virtual Machine) to solve issues with smart contracts and WASM Variables declared as interface always creates a pointer Scope != Allocation (for loop vars) Go reflection is limited Lightning Talks Standardizing Errors in Go: A Practical Guide with Dapr | Cassie Coyle, Diagrid Richer Error Model: code, message, details Errors messages should provide as much information as possible, evenn resources to help fix the problem Dapr provides an error package that can be used to create errors with rich details Chan Chan Chan T: A Generic Tale | Branden J. Brown finding a match in communication, talking to other goroutines communication deduplicates state Go\u0026rsquo;s use in Competitive Programming CP is answering questions via code usually taken in STDIN and STDOUT Pros: \u0026hellip;? Its fast? Cons: Compiler support, problems arent suitable to parallelizing, stdlib lacks in DSA, verbose CPP is anti-software engineering (its throw-away code), it is not a good fit for Go 7 Surprising Features of the Go Playground | Matt Dale, MongoDB support for pulling in modules allows to create multiple files (go.mod) ---filepath--- can write tests in the playground clear the terminal by prinitng fmt.Println(\u0026quot;\\x1b[2J\\x1b[H\u0026quot;) display images goplay.tool goplay.space Saving African Savannah Wildlife with Go-MQTT - Marvin Hosea Decided to use Go for the backend, because it is a good language for IoT MQTT is a lightweight pub/sub protocol, used for IoT -\u0026gt; use the Mosquitto broker (free!) This Go MQTT client library was not very good, hand bugs and not very well tested, speaker decided to work on it himself The end\u0026hellip; TLDR; Short speach, but a good talk for a even better cause. Cool how Go can be used to solve real world problems, IOT is a big one. Building a promgrammable firewall with Go - Mason Johnson Firewalls are a very important part of any network, they are used to prevent unwanted traffic Using go packages (netlink, nftables, firewall_toolkit) Get netlink connection using netlink package Add a table using the netlink connection (need to call .Flush()) Add a chain to the table and Flush create a new rule using firewall_toolkit lib Can create a set to store the mutilple IP Can connect via Kafa (sends blocked IP\u0026rsquo;s) and send the blocked IP\u0026rsquo;s to the firewall updating the blocked IP\u0026rsquo;s set TLDR; Go is great for Network programing, can program a firewall with an ergonomic API Automating Efficiency Improvement by Profile Guided Optimization - Jin Lee Uber has over 26000 Go microservices PGO is a compiler optimization technique that can be used to improve performance of Go programs Looks like a pretty complex process\u0026hellip; Go-json PGO inlining saw improvments of 12% in performance PGO originally exptended compile times by 4x, until PGO pre-processing was introduced TLDR; PGO is a great tool to improve performance of Go programs, but it is a complex process. Prototyping a Go, GraphQL, gRPC Microservice - Sadie Freeman Should always start with a prototype, articulate the problem what stack, why, etc Documents the process (pros-cons) Use existing libs/frameworks Use useful logs (!) Don\u0026rsquo;t overcomplicate, make assumptions Archetcture - inlcudes two services one for creating Alpacha and another for analytics each with their own DB Why GraphQL? More flexible querying, front-end friendly Fewer server requets Powerful pagination Define proto file schema and generate code (resolver) gRPC (service to service communication) Fast \u0026amp; Efficent HTTP/2 Protocol Buffers (Binary serialization) Clear Proto schmea Can quickly get up and running with this stack TLDR; Cool talk, very practical and useful, would like looking for into in with GraphQL, gRPC, and Buf Closing Talk (Day 1): A Decade of Debugging - Derek Parker Go debuggers were immature, did not track goroutines properly in 2014 Delve was created to address this issue by Derek Q? Who uses this? Neovimers? \u0026hellip;No everyone! It is used behind the scenes for all these debuggers TLDR; mostly a talk about the delve debugger and change log throught out the 10 years\u0026hellip;kinda cool debuggers are interesting Day 2 Who Tests the Tests? - Daniela Petruzalek A way to validate tests - Rotation Testing Why do we write tests? Tests are our insurence policy The code does what its supposed to do, happy path :) The code doesnt do what its not supposed to do, sad path :( Tests are also design tools First clients of our code Strong correlation between code that is easy to test and code that is easy to read/maintain Measauring tests Code coverage Test coverage Goodharts Law - A metric that becomes a goal stops being a good metric Test coverage is easily faked You can test tests via mutations, esentially mutate the code you are testing and it should fail. This is mutant tests coverage. In Go you can use an access the AST to program these mutations to happen during runtime Write tests for the right reasons, choose your test inputs wisely, mutation testing in go is not mature yet TLDR; very interesting talk, first time hearing of Rotation/Mutation testing, however seems like alot of work to get running Interface Internals - Keith Randall Multiply simply asks the machine to multiply - nothing needed by Go compiler For interfaces there needs to be work\u0026hellip;find the contained type..get list of methods..etc Can be done in 3 machine code instructions Interface variables bridge the gap between static and dynamic worlds, it can be reassigned Interfaces in Go are just 2 word structs with a pointer to the interface type and a pointer to the interface data Utilize an interface tab on interface data pointer to store addiotnal data -\u0026gt; all methods of an interface (sorted order) fun[0] TLDR; Very interesting talk, very useful to know how interfaces are implemented in Go and how the comiler converts to machine code Building a High-Performance Concurrent Map in Go - YunHao Zhang Go\u0026rsquo;s built in map is not concurrent safe sync.Map is provided by stdlib using built in map with a sync.RWMutex (together in a struct?) sync.Map is optimized for cahce contention - only fast to reads For RWMap there is a bottle neck for writes (blocks all reads) We can create a shared map = multiple read-write mutex maps Sounds very similar to databse sharding but for maps, writes will only block the map it is writing too Hash function to know which map to write/read to In all cases this implementation is faster, however takes more memory Can create a sub-map each new key-value pair creates a new node and all nodes are connected via pointers (LinkedList) SkipList = multiple linked lists SkipMap is 20x faster in typical cases however it is O(logn) on read/writes, sync.Map can be faster in low-currency cases TLDR; cool talk about data structures and system design, kina got lost towards the end but very interesting The Go Cryptography State of the Union - Fillipo Valsorda Post Quantum Cryptography is about the future, these computers are powerful and can crack these cryptographic algorithms Hyribs of new and old implementations to solve both problems New lib crypto/internal/mlkem768 300 lines made for readability fast and low memory footprint cryoto/tls updated defaults in 1.24 encrypted client hello in 1.23 math/rand should not be used for security before version 1.22 ChaCha8Rand is now uses in rand packages to make secure still should use crypto/rand these apis shold no longer return erros as they are now deterministic ssh package is catching up cool new API secret.Do that takes a closure and code is cleared from stack after done TLDR; cryptography is hard, Go maintainers are up for the challenge and the libs are battle tested and improving Advanced Code Instrumentation Techniques for High-Performance Trading Systems - Holly Casaletto \u0026ldquo;You can observe a system, but be prepared for it to act like it knows your watchinig!\u0026rdquo; A trading needs to be highly durable and performant (high throughout - low latency - concurrent) Merics API where we need to handle locking, etc TLDR; was kinda tired for this one, was very verbose and a bit too much info for me Digital Audio from Scratch - Patrick Stephen Finally some music using a lib daw we are sending bytes into speaker -\u0026gt; higher byte = higher volume, pattern = frequency speaker is going through different waves/numbers/and math to get a cool sound TLDR; Music is cool and interesting (also complicated), you can do it in Go if you want.. Lightning Talks How to Mock Your Coworkers - Dylan Bourque from CrowdStrike Mocking is test doubling - you can mock out the dependencies of a function Don\u0026rsquo;t export your mocks\u0026hellip;write(or generate) your mocks internal to your project \u0026hellip;dont actaully mock your coworkers lol Lightning-Fast Database Tests - Robin Hallabro-Kokko DB tests are usually the slowest tests Fast tests lower the barier for creating new tests (CI/CD, faster development) pgtestdb creates emphemal postgres databases quickly and handles lifecycle it is very quick and seems easy to use Built it! With or WIthout Tools - Matthew Sanabria Speaker compares refactoring production code to building his new kitchen Right Tools, Wrong Knowledge - Learn! Read the docs Wrong Tools, Write Tests - It happends, verify tools work by writing tests Know when to seek an expert, and observe their work Greatest tool in your toolbox is you You can store that in a container registery? - Siva Manivannan Yes, you can store things that are not containers in a container registry Container Registry is metadata and a blob If you implment these ~20 API endpoints you have a container registry Would You Like a GUI With That? - Andy Joseph, ProntoGUI ProntoGUI is a lib that allows to build GUI\u0026rsquo;s in Go Events from frontend to backend are via gRPC Kinda cool? Don\u0026rsquo;t see how its better/worse than other GUI libs Embracing the Replace: Our Journey to Fixing the Plugin System in HashiCorp Packer - Wilken Rivera, HashiCorp Company had a breaking change bc they were using a experimental package that eventaully was dropped Options were limited, decided to fork the package and fix it added a CLI command to fix the issue for their users when having a breaking change, stop the bleed, and do whats best for users even if its the hardest A 10-Year Retrospective on Building an Infrastructure Start-up in Go - Alan Shreve ngrok originall use was to have public url for local project running on a port when started in 2014 go was still very immature in problems related to ngrok sqlc for database stuff Use lots of gRPC over ~100 services Interceptors (like middleware but for gRPC) can be used on client and server side use RichErrors (custom Error type struct) and have a client side interceptor that catches this Replicate all the things Integration Testing - matrix of thosands of tests run in production 24/7 in a loop (wow) Define your errors to help users ngrok\u0026rsquo;s success is acredited to Go and its proerties dont get attached to code software is meant to be rewritten Processing Millions of Events Per Second Reliably Using Generics - Alan Braithwaite Stream processing processes a continouts stream of distint events consuming from some source external to the system and delivering to some desination external to the system Goals for the project, be fast, plugins are easy to create, support any serializable type, batteries included, at least one delivery Okay this is where generics comes in, using the Go lib kawa the types are used as generics Speaker argues against using channels due to their overhead and unseen complexity TLDR; stream proccessing is very interesting, i want to look into this. Project seems cool Building a Self-Regulating Pressure Relief Mechanism in Go - Ellen Gao it is importnat for systems to have healthy utlization (cpu?) Not always best option to just increase resources, (cost, complexity) A presure valave can detect a soft limit and perform action slow down processing of non-critical processes, these messges are processed later Program uses a messague broker pub/sub system to mimic opearations and priority Basic throttler implemented with time.Sleep program now completes Adds 3 different throttling strategies (block, fixed delay, and backoff) for each priority via interfaces Then implements recovery, most was already implemented so not much changes needed TLDR; interesting talk, good speaker, learned some about throttling and pressure relief, liked the the flow from talk to code. Go in the Smallest of Places - Patricio Whittingslow] First major software was the Apollo spaceship and Margarette Lewis was first SWE, written in assembly C was created a higher level language (comparatievly) for micro controllers TinyGo makes Go usuable for MCU development Working with MCU\u0026rsquo;s is verbose, you needd to shift a bunch of bits in memory (registers) TLDR; TinyGo is awesome and I need to use it with the programabale badge I have Range Over Function Types - Ian Lance Taylor New feature in 1.23 release Ways to loop over all of an elements in a Set in Go Extend the for range, or add iterator type, how about both .All() method should be created for container types which returns an iterator TLDR; finally iterators in go, altough don\u0026rsquo;t love the implementation End of Day 2 ","permalink":"http://localhost:1313/posts/gophercon/","summary":"GopherCon 2024 Notes Talks Day 1 Charm CLi (Habbit Tracker) - Donia Chaiehloudj Charm is a teminal UI written in go uses ELM archetecure - Model, View, Update grpC backend? (To store habbits etc) check out book Learning Go with pocket sized projects TLDR;Pretty cool, have used before not very succesfully, if I work with the Architecture of the project, I think it will be a lot easier to work with.","title":"Gophercon 2024 Note Dump"},{"content":"This is my first Hugo site it seems pretty cool\u0026hellip;.\nSet up theme PaperMod, and menu bar along with tags and a fuzzy search. I like how Hugo is written in Go, very interested in learning more about it (possibly Contribute to it?), anyways looking forward to making more posts. Kaizen! ","permalink":"http://localhost:1313/posts/my-first-post/","summary":"This is my first Hugo site it seems pretty cool\u0026hellip;.\nSet up theme PaperMod, and menu bar along with tags and a fuzzy search. I like how Hugo is written in Go, very interested in learning more about it (possibly Contribute to it?), anyways looking forward to making more posts. Kaizen! ","title":"My First Post"}]